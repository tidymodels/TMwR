```{r dimensionality-setup, include = FALSE}
library(tidymodels)
library(ggforce)
library(embed)
library(corrplot)
library(learntidymodels)
library(baguette)
library(discrim)
library(uwot)
library(doMC)

registerDoMC(cores = parallel::detectCores())
tidymodels_prefer()
load("RData/dry_beans.RData")
```

# Dimensionality reduction  {#dimensionality}

Dimensionality reduction is a nice tool for situations when there is suspicion that there are "too many" variables. An excess of variables, usually predictors, can be problematic since it may be difficult to understand or visualize data in higher dimensions. For example, in high dimensional biology experiments, one of the first tasks is to determine if there are any unwanted trends on the data (e.g., effects not related to the question of interest, such as lab-to-lab differences). Debugging the data is difficult when there are hundreds of thousands dimensions. Dimensionality reduction can be a great aid for exploratory data analysis. 

Another potential consequence of having a multitude of predictors is the potential harm that it can have on a model. The simplest example is a method like ordinary linear regression where the number of predictors should be less than the number of data points used to fit the model. Another issue is multicollinearity where between-predictor correlations can negatively impact the mathematical operations used to estimate a model. If there are an extremely large number of predictors, it is fairly unlikely that there are an equal number of underlying effects. Many predictors may be measuring the same latent effect(s) and the result is a high degree of correlation between predictors. Many dimensionality reduction techniques thrive on this situation. In fact, most can only be effective when there are relationships between predictors that can be exploited. 

When starting a new project, reducing the dimensions of the data may also provide some intuition on how hard the modeling problem may be. For example, principal component analysis (PCA) is one of the simplest methods for reducing the number of columns in the data set because it relies on linear methods and it is unsupervised (i.e., does not consider the outcome data). For a high dimensional classification problem, an initial plot of the main PCA components might show a clear separation between the classes. If this is the case, then it is fairly safe to assume that a linear classifier might be enough to do a good job on the data. However, the converse is not true; a lack of separation does not mean that the problem is insurmountable. 

Finally, the dimensionality reduction methods discussed here are generally not feature selection methods. Methods such as PCA represent the original predictors using a smaller subset of new features. All of the original predictors are required to compute these new features. The exception to this are sparse methods that have the ability to completely remove the impact of predictors when creating the new features. 

This chapter uses an example data set to demonstrate how tidymodels packages, especially recipes, can aid dimensionality reduction. The initial focus is on visualization then we segue into using a few methods with models. In addition to the `r pkg(tidymodels)` package, this chapter uses the following packages: `r pkg(baguette)`, `r pkg(bestNormalize)`, `r pkg(corrplot)`, `r pkg(discrim)`, `r pkg(embed)`, `r pkg(ggforce)`, `r pkg(klaR)`, `r pkg(learntidymodels)`,  `r pkg(mixOmics)`, and  `r pkg(uwot)`. `r pkg(learntidymodels)` can be found at its [GitHub site](https://github.com/tidymodels/learntidymodels). 

To get started, we'll consider a data set that might benefit from dimension reduction. 

## A picture is worth a thousand beans {#beans}

@beans describe methods for determining the varieties of dried beans in an image. From their manuscript: 

> The primary objective of this study is to provide a method for obtaining uniform seed varieties from crop production, which is in the form of population, so the seeds are not certified as a sole variety. Thus, a computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera.

The images are processed so that the beans can be analyzed. Once there is data in each bean in the images, a set of images are manually labeled and these are used to create a predictive model that can distinguish between seven varieties: Cali, Horoz, Dermason, Seker, Bombay, Barbunya, and Sira. Producing an effective model can help manufacturers quantify how homogeneous a batch of beans is. 

For image data, there are a multitude of analysis methods available for use. One is the use image analysis to segment the individual beans and then quantify them using features related to color and morphology (i.e., shape). These features are then used to model the outcome with the idea that different bean varieties look different. 

For example, there are numerous methods to quantify aspects of shapes of objects [@Mingqiang08]. Many are related to the boundaries or regions of the object of interest. Example of features include: 

* The _area_ (or size) can be estimated using the number of pixels in the object or the size of the convex hull around the object. 

* We can measure the _perimeter_ using the number of pixels in the boundary as well as the area of the bounding box (the smallest rectangle enclosing an object).

* The major axis quantifies the longest line connecting the most extreme parts of the object. The minor axis uses to the shortest line. 

* We can measure the _compactness_ of an object using the ratio of the object's area to the area of a circle with the same perimeter. For example, the symbols "`r cli::symbol$lower_block_4`" and "`r cli::symbol$times`" have very different compactness. 

* There are also different measures of how oblong or _elongated_ an object is. For example, the _eccentricity_ statistic is the ratio of the major and minor axes. There are also related estimates for roundness and convexity. 

As an example, here are different shapes and their corresponding eccentricity:

```{r dimensionality-eccentricity, echo = FALSE, out.width="95%"}
knitr::include_graphics("figures/morphology.svg")
```

Note that shapes such as circles and squares have low eccentricity while oblong shapes have high values. Also, the metric is unaffected by the rotation of the object. 

Many of these image features have high correlations; objects with large areas are more likely to have very low eccentricity. As noted above, there are often multiple methods to quantify the same underlying characteristics (e.g. size). 
Many of these image features have high correlation; objects with large areas are more likely to have very low eccentricity. As noted above, there are often multiple methods to quantify the same underlying characteristics (e.g. size). 

In the bean data, `r ncol(dry_beans) - 1` morphology features were computed: `r knitr::combine_words(gsub("_", " ", names(dry_beans)[-ncol(dry_beans)]))`. The latter four are described in @symons1988211. While the dimensionality of these data is not large, it does provide a nice working example to demonstrate how to reduce the number of features. 

The data are loaded using

```{r dimensionality-import}
library(tidymodels)
tidymodels_prefer()
load("RData/dry_beans.RData")
```

It is important to maintain good data discipline when evaluating dimensionality reduction techniques (especially for their use within a model). For our analyses, a test set is created. The remaining data are  split into training and validation sets:

```{r dimensionality-split}
set.seed(1701)
bean_split <- initial_split(dry_beans, strata = class, prop = 3/4)

bean_train <- training(bean_split)
bean_test  <- testing(bean_split)

set.seed(1702)
bean_val <- validation_split(bean_train, strata = class, prop = 4/5)
bean_val$splits[[1]]
```

To assess how well different methods perform (visually), the methods are estimated on the training set (n = `r analysis(bean_val$splits[[1]]) %>% nrow()` beans) and displayed using the validation set (n = `r assessment(bean_val$splits[[1]]) %>% nrow()`). 

## Exploratory data analysis

We should probably spend some time investigating our data. Knowing that the image features are probably measuring similar concepts, let's take a look at the correlation structure of the data: 

```{r dimensionality-corr-plot, fig.height=6, fig.width=6, out.width="70%"}
library(corrplot)
bean_train %>% 
  select(-class) %>% 
  cor() %>% 
  corrplot()
```

While we don't take the time to do it here, it is also important to see if this structure significantly changes across the outcome categories. This can be insightful and help create better models. 

The bulk of our time will be spent on looking at the data in a smaller space. We can create a basic recipe to preprocess the data prior to the addition of dimensionality reduction steps. A number of predictors are ratios and these are likely to have skewed distributions. Such distributions might wreak havoc on variance calculations (such as the ones used in PCA).  The `r pkg(bestNormalize)` package has a step that can enforce a symmetric distribution for the predictors. We'll use this to mitigate the issue of skewed distributions. 

```{r dimensionality-initial-rec}
library(bestNormalize)
bean_rec <-
  recipe(class ~ ., data = analysis(bean_val$splits[[1]])) %>%
  step_zv(all_numeric_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  prep()
```

Since recipes are the primary method in tidymodels for dimensionality reduction methods, let's write a function that will estimate the transformation and plot the resulting data in a scatter plot matrix via the `r pkg(ggforce)` package: 

```{r dimensionality-function}
library(ggforce)
plot_validation_results <- function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe %>%
    prep() %>%
    bake(new_data = dat) %>%
    ggplot(aes(x = .panel_x, y = .panel_y, col = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = .3) +
    facet_matrix(vars(-class), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
}
```

This will be reused several times below. 

A series of several methodologies are used here. An overview of most can be found in [Section 6.3.1 ](https://bookdown.org/max/FES/numeric-many-to-many.html#linear-projection-methods) of @fes and the references therein. The UMAP method is described in @mcinnes2020umap.

### Principal component analysis

As previously mentioned, PCA is an unsupervised method that uses linear combinations of the predictors to define new features. These features attempt to account for as much variation as possible in the original data. We add `step_pca()`  to the original recipe and use our function to visualize the results on the validation set:

```{r dimensionality-pca, dev = "png", fig.height=7}
bean_rec %>%
  step_pca(all_numeric_predictors(), num_comp = 4) %>%
  plot_validation_results() + 
  ggtitle("Principal Component Analysis")
```

We see that the first two components, especially when used together, can do an effective job separating the classes. This may lead us to believe that the overall problem of classifying these beans will not be especially difficult. 

Recall that PCA is unsupervised. For these data, it turns out that the PCA components that explain the most variation in the predictors also happen to be predictive of the classes. What features are driving performance? The `r pkg(learntidymodels)` package, found on GitHub, has functions that can help visualize the top features for each component. We'll need the prepared recipe; the PCA step is added below along with a call to `prep()`: 

```{r dimensionality-pca-loadings}
library(learntidymodels)
bean_rec %>%
  step_pca(all_numeric_predictors(), num_comp = 4) %>% 
  prep() %>% 
  plot_top_loadings(component_number <= 4, n = 5) + 
  ggtitle("Principal Component Analysis")
```

The top loadings are mostly related to the cluster of correlated predictors shown  in the top left portion of the previous correlation plot: perimeter, area, major axis length, and convex area. These are all related to bean size. Shape factor 2, from @symons1988211, is the area over the cube of the major axis length and is therefore  also related to bean size. Measure of elongation appear to dominate the second PCA component.  

### Partial least squares

PLS is a supervised version of PCA. It tries to find components that simultaneously maximize the variation in the predictors while also maximizing the relationship between those components and the outcome. 

```{r dimensionality-pls, dev = "png", fig.height=7}
bean_rec %>%
  step_pls(all_numeric_predictors(), outcome = "class", num_comp = 4) %>%
  plot_validation_results() + 
  ggtitle("Partial Least Squares")
```

Interestingly, the first two PLS components are identical to the first two PCA components (due to their effectiveness at separating the varieties of beans). The remaining components are different. 

```{r }
library(learntidymodels)
bean_rec %>%
  step_pls(all_numeric_predictors(), outcome = "class", num_comp = 4) %>%
  prep() %>% 
  plot_top_loadings(component_number <= 4, n = 5, type = "pls") + 
  ggtitle("Partial Least Squares")
```

Solidity (i.e., the density of the bean) drives the third PLS component, along with roundness. Solidity may be capturing bean features related to "bumpiness" of the bean surface since it measures irregularity of the bean boundaries.  Extent is the largest effect in the fourth component; it is an image texture measure similar to solidity. 

### Independent component analysis

ICA is  slightly different than PCA in that it finds components that are as statistically independent from one another as possible (as opposed to being uncorrelated). It can be thought of as maximizing the "non-Gaussianity" of the ICA components.

```{r dimensionality-ica, dev = "png", fig.height=7}
bean_rec %>%
  step_ica(all_numeric_predictors(), num_comp = 4) %>%
  plot_validation_results() + 
  ggtitle("Independent Component Analysis")
```

To the bare eye, there does not appear to be any separation between the classes when using ICA. 

### Uniform manifold approximation and projection

UMAP is similar to the popular t-SNE method for nonlinear dimension reduction. In the high-dimensional space, UMAP uses a distance-based nearest neighbor method to find local areas of the data where the data points are more likely to be related. The relationship between data points is saved as a directed graph model where most points are not connected. 

From there, UMAP translates points in the high dimensional graph to the reduced dimensional space. To do this, the algorithm has an optimization process that uses cross-entropy to map data points to the smaller set of features so that the data in the graph is well approximated. 

To create the mapping, the `r pkg(embed)` package contains a step function for this method: 

```{r dimensionality-umap, dev = "png", fig.height=7}
library(embed)
bean_rec %>%
  step_umap(all_numeric_predictors(), num_comp = 4) %>%
  plot_validation_results() +
  ggtitle("Uniform Manifold Approximation and Projection")
```
While the between-cluster space is pronounced, the clusters can contain a heterogeneous mixture of classes.  

There is also a supervised version of UMAP: 

```{r dimensionality-umap-supervised, dev = "png", fig.height=7}
bean_rec %>%
  step_umap(all_numeric_predictors(), outcome = "class", num_comp = 4) %>%
  plot_validation_results() +
  ggtitle("Uniform Manifold Approximation and Projection (supervised)")
```

The supervised method shows promise for modeling the data. 

UMAP is a powerful method to reduce the feature space. However, it can be very sensitive to tuning parameters (e.g. the number of neighbors and so on).  For this reason, it would help to experiment with a few of the parameters to assess how robust the results are for these data. 


## Modeling {#bean-models}

Both the PLS and UMAP methods are worth investigating in conjunction with different models. A variety of different models are explored with these dimensionality reduction techniques (along with no transformation): a single layer neural network, bagged trees, flexible discriminant analysis (FDA), naive Bayes, and regularized discriminant analysis (RDA). 

We'll create a series of model specifications and then use a workflow set to tune the models. Note that the model parameters are tuned in conjunction with the recipe parameters (e.g. size of the reduced dimension,  UMAP parameters). 

```{r dimensionality-models}
library(baguette)
library(discrim)

mlp_spec <-
  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
  set_engine('nnet') %>%
  set_mode('classification')

bagging_pec <-
  bag_tree() %>%
  set_engine('rpart') %>%
  set_mode('classification')

fda_spec <-
  discrim_flexible(
    prod_degree = tune()
  ) %>%
  set_engine('earth')

rda_spec <-
  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %>%
  set_engine('klaR')

bayes_spec <-
  naive_Bayes() %>%
  set_engine('klaR')
```


```{r dimensionality-recipes}
bean_rec <-
  recipe(class ~ ., data = bean_train) %>%
  step_zv(all_numeric_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors())

pls_rec <- 
  bean_rec %>% 
  step_pls(all_numeric_predictors(), outcome = "class", num_comp = tune())

umap_rec <-
  bean_rec %>%
  step_umap(
    all_numeric_predictors(),
    outcome = "class",
    num_comp = tune(),
    neighbors = tune(),
    min_dist = tune()
  )
```

Once again, `r pkg(workflowsets)` take the preprocessors and models and cross them. The `control` option `parallel_over` is set so that the parallel processing can work simultaneously across  tuning parameter combinations. `workflow_map()` applies grid search to optimize the model/preprocessing parameters (if any) across 10 parameter combinations. The multi-class area under the ROC curve is estimated on the validation set. 


```{r dimensionality-workflows}
ctrl <- control_grid(parallel_over = "everything")
bean_res <- 
  workflow_set(
    preproc = list(basic = class ~., pls = pls_rec, umap = umap_rec), 
    models = list(bayes = bayes_spec, fda = fda_spec,
                  rda = rda_spec, bag = bagging_pec,
                  mlp = mlp_spec)
  ) %>% 
  workflow_map(
    verbose = TRUE,
    seed = 1703,
    resamples = bean_val,
    grid = 10,
    metrics = metric_set(roc_auc)
  )
```

The models are ranked by their validation set estimates of the area under the ROC curve: 

```{r dimensionality-rankings}
rankings <- 
  rank_results(bean_res, select_best = TRUE) %>% 
  mutate(method = map_chr(wflow_id, ~ str_split(.x, "_", simplify = TRUE)[1])) 

tidymodels_prefer()
filter(rankings, rank <= 5) %>% dplyr::select(rank, mean, model, method)

rankings %>% 
  ggplot(aes(x = rank, y = mean, pch = method, col = model)) + 
  geom_point(cex = 3) + 
  theme(legend.position = "right") +
  labs(y = "ROC AUC")
```

It is clear from these results that most models give very good performance; there are few bad choices here. For demonstration, the RDA model with PLS features is used as the final model. We'll finalize the workflow with the numerically best parameters, fit it to the training set, then evaluate the test set:

```{r dimensionality-final}
rda_res <- 
  bean_res %>% 
  pull_workflow("pls_rda") %>% 
  finalize_workflow(
    bean_res %>% 
      pull_workflow_set_result("pls_rda") %>% 
      select_best(metric = "roc_auc")
  ) %>% 
  last_fit(split = bean_split, metrics = metric_set(roc_auc))

rda_wflow_fit <- rda_res$.workflow[[1]]
```

```{r dimensionality-test}
collect_metrics(rda_res)
```

Pretty good! We'll use this model in the next chapter to demonstrate variable importance methods. 

```{r dimensionality-save, include = FALSE}
save(rda_wflow_fit, bean_train, file = "RData/rda_fit.RData", version = 2, compress = "xz")
```

## Chapter summary {#dimensionality-summary}

Dimensionality reduction can be a helpful tool for exploratory data analysis as well as modeling. The `r pkg(recipes)` and `r pkg(embed)` packages contain steps for a variety of different methods and `r pkg(workflowsets)` facilitates choosing an appropriate method for a data set. 
